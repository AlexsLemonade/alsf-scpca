---
title: "Pre Processing Metrics Benchmarking"
output: html_notebook
---

This notebook is a follow up notebook to previous benchmarking done in `02-compare-quants.RMD`. 
Here, we have used the most recent release of [Salmon](https://github.com/COMBINE-lab/salmon/releases/tag/v1.4.0) and 
[Cellranger 6.0.0](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/release-notes), and included the new [Alevin-Fry](https://alevin-fry.readthedocs.io/en/latest/getting_started.html) method. 
Alevin-Fry can use either a selective alignment strategy or pseudoalignment strategy (with the `--sketch` parameter),
so here we are testing both strategies. The resolution currently used for Alevin-Fry is the Full resolution. See the documentation for more information on different options for resolution that could be used. 

The goal of this notebook is to compare the quantification performed by each tool across the samples we have currently run. Thus far, we have 4 10Xv3 samples that have been run on all tools, SCPCR000003, SCPCR000006, SCPCR000126, and SCPCR000127. Next we will need to incorporate more samples from different technologies and from snRNA-sequencing that will be added to this benchmarking, as we have samples from all categories that will be analyzed eventually. 

## Setup 

### Load Libraries for import

```{r setup}
library(tidyverse)
library(ggplot2)
library(SingleCellExperiment)
library(forcats)
```

### File and Directory Setup

```{r}
# where all the data lives after running 01-import-quant-data.Rmd
import_dir <- file.path('data')

quant_info_file <- file.path(import_dir, 'quant_info.tsv')

alevin_rds <- file.path(import_dir, 'alevin_sces.rds')
alevin_fry_rds <- file.path(import_dir, 'alevin_fry_sces.rds')
alevin_fry_sketch_rds <- file.path(import_dir, 'alevin_fry_sketch_sces.rds')
kallisto_rds <- file.path(import_dir, 'kallisto_sces.rds')
cellranger_rds <- file.path(import_dir, 'cellranger_sces.rds')
```


```{r}
# read in quant info file created in 01-import-quant-data.Rmd
quant_info <- readr::read_tsv(quant_info_file)

# make sure that the tool is labeled as alevin_fry_sketch to distinguish from regular alevin_fry for later
quant_info[which(quant_info$alevin_alignment == "sketch"),"tool"] <- "alevin_fry_sketch"
```


```{r}
# Each RDS file contains a SingleCellExperiment
alevin_sces <- readr::read_rds(alevin_rds)
alevin_fry_sces <- readr::read_rds(alevin_fry_rds)
alevin_fry_sketch_sces <- readr::read_rds(alevin_fry_sketch_rds)
kallisto_sces <- readr::read_rds(kallisto_rds)
cellranger_sces <- readr::read_rds(cellranger_rds)
```

```{r}
## get all annotation data from S3
annotation_dir <- file.path(import_dir, "annotation")

## get mitochondrial gene list from s3
annotation_files_s3 <- 
  's3://nextflow-ccdl-data/reference/homo_sapiens/ensembl-100/annotation/'

sync_call <- paste('aws s3 sync', annotation_files_s3, annotation_dir)
system(sync_call, ignore.stdout = TRUE)
```

```{r}
mito_file <- file.path(annotation_dir, "Homo_sapiens.ensembl.100.mitogenes.txt")

mito_genes <- readr::read_tsv(mito_file, col_names = "gene_id")
mito_genes <- mito_genes %>%
  pull(gene_id)
```



## Add QC Metrics

```{r}
addPerCellQC_mito <- function(sce, mito = mito_genes){
  # add per cell QC with mitochondrial genes separately for later comparisons
  scater::addPerCellQC(
    sce, 
    subsets = list(mito = mito[mito %in% rownames(sce)])
  )
}
```

```{r}
alevin_sces <- alevin_sces %>% 
  purrr::map(addPerCellQC_mito)

alevin_fry_sces <- alevin_fry_sces %>%
  purrr::map(addPerCellQC_mito)

alevin_fry_sketch_sces <- alevin_fry_sketch_sces %>%
  purrr::map(addPerCellQC_mito)

kallisto_sces <- kallisto_sces %>% 
  purrr::map(addPerCellQC_mito)

cellranger_sces <- cellranger_sces %>% 
  purrr::map(addPerCellQC_mito)

```


```{r}
## merge all QC output into one data frame to work with for plotting comparisons
celldata_to_df <- function(sce){
  # extract the column (cell) summary data from a SCE, 
  # convert to data frame and move cell id to a column
  as.data.frame(colData(sce)) %>%
  tibble::rownames_to_column(var = "cell_id") %>%
    # add a column to get # of cells detected in that sample
    mutate(cells_detected = ncol(sce),
           # add genes per cell for each sample
           genes_detected = nrow(sce))
}

alevin_cell_qc <- alevin_sces %>%
  purrr::map_df(celldata_to_df, .id = "quant_id")

alevin_fry_cell_qc <- alevin_fry_sces %>%
  purrr::map_df(celldata_to_df, .id = "quant_id")

alevin_fry_sketch_cell_qc <- alevin_fry_sketch_sces %>%
  purrr::map_df(celldata_to_df, .id = "quant_id")

kallisto_cell_qc <- kallisto_sces %>%
  purrr::map_df(celldata_to_df, .id = "quant_id")

cellranger_cell_qc <- cellranger_sces %>%
  purrr::map_df(celldata_to_df, .id = "quant_id")

# combine all the data frames into one
cell_qc <- dplyr::bind_rows(
  alevin = alevin_cell_qc,
  alevin_fry = alevin_fry_cell_qc,
  alevin_fry_sketch = alevin_fry_sketch_cell_qc,
  kallisto = kallisto_cell_qc,
  cellranger = cellranger_cell_qc,
  .id = "tool"
) %>%
  dplyr::left_join(quant_info,
                   by = c("tool" = "tool", 
                          "quant_id" = "quant_dir"))
```



## Comparison of QC Metrics across all tools

### Per Cell QC

Let's first start by taking a look at some general information across all of our tools and how consistent each of these metrics is for all four of the samples. Here are the metrics we want to look at: 

  * Total number of cells detected
  * Mitochondrial content/ cell
  * Number of UMIs/cell
  * Number of Genes/cell 


```{r}
## make new data frame with mean of cells detected and standard dev
cells_detected_stats <- cell_qc %>%
  group_by(sample, tool) %>%
  summarise(cells_detected = mean(cells_detected)) %>%
  group_by(tool) %>%
  summarise(mean = mean(cells_detected),
            standard_error = sd(cells_detected)/sqrt(length(cells_detected)))

## first look at cells detected
ggplot(cells_detected_stats, aes(x = tool,y = mean)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  geom_errorbar(aes(x = tool, ymin=mean-standard_error, 
                    ymax = mean + standard_error), 
                width = 0.4, colour = "black", alpha = 0.9, size = 1.3)+
  theme_classic() +
  ylab("Total Cells Detected") + 
  xlab("")
```
Based on this, we can see that Cellranger is detecting what looks like the most number of cells with Alevin the least number of cells. However, Cellranger also looks like it has the largest variation, and all of the tools appear to have quite a bit of variation. It does make sense to have variation because it's likely that samples just have lower cells than other samples. Let's look at a plot that may be a little more informative and break down the number of cells detected/sample across all the tools. 


```{r}
cells_detected_stats <- cell_qc %>%
  group_by(sample, tool) %>%
  summarise(cells_detected = mean(cells_detected))

## now break it down by sample 
ggplot(cells_detected_stats, aes(x = tool, y = cells_detected)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~sample) +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("Number of Cells Detected") + 
  xlab("")
```
Okay, now we can see that even within one sample there is quite the variation across the tools. This seems a bit concerning... Cellranger looks like it detects the most amount of cells for 2 of the 4 samples and Alevin for the other 2. But then in the 2 samples (126 and 127) that Cellranger has the most, Alevin has the least? Is there something potentially wrong with those runs with alevin? 

Let's take a look at the mitochondrial content per cell across the tools and see if we see variation there. 
```{r}
## mitochondrial content 
cell_qc %>%
  mutate(tool = fct_reorder(tool, subsets_mito_percent, .fun = 'mean')) %>%
  ggplot(aes(x = reorder(tool, subsets_mito_percent), y = subsets_mito_percent, fill = tool)) + 
  geom_boxplot() + 
  theme_classic() + 
  ylab("Mitochondrial content") + 
  xlab("")
```
So it does appear like cellranger outputs cells with the lowest mitochondrial content in comparison to the other tools. However, the very high range in Alevin makes me concerned. Is this perhaps an issue with those individual runs? Let's break it down by sample.

```{r}
ggplot(cell_qc, aes(y = subsets_mito_percent, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) + 
  theme_classic() + 
  ylab("% Mito /Cell") + 
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
```
So this is really informative. It looks like SCPCR000003 is a very poor sample across all tools and has a lot of dead cells. It might be a good idea to remove that from our comparisons and not consider this for benchmarking. Additionally, it looks like we can see where our outlier in Alevin is, SCPCR000006, has a much higher mito content in the Alevin run than in other tools. Let's see if other metrics are affected with that sample. 

```{r}
# remove SCPCR000003 from cell_qc for the rest of the plotting 
cell_qc_filter <- cell_qc %>%
  dplyr::filter(sample != "SCPCR000003")
```

Now let's look at the number of UMI's detected/cell that we've removed our outlier sample. 

```{r}
cell_qc_filter %>%
  mutate(tool = fct_reorder(tool, sum, .fun = 'mean')) %>%
  ggplot(aes(x = reorder(tool, sum), y = sum, fill = tool)) + 
  geom_boxplot() + 
  theme_classic() + 
  ylab("Number of UMI/Cell") + 
  xlab("")
```
From this, it looks like Cellranger has the lowest UMI's/cell with Alevin and kallisto very close to each other at the top. Alevin does have quite a large variation though. 

Out of curiosity, how would it look if we kept that sample in that had really high mito content. 

```{r}
cell_qc %>%
  mutate(tool = fct_reorder(tool, sum, .fun = 'mean')) %>%
  ggplot(aes(x = reorder(tool, sum), y = sum, fill = tool)) + 
  geom_boxplot() + 
  theme_classic() + 
  ylab("Number of UMI/Cell") + 
  xlab("")
```
You can see that the one outlier has very low UMI's/cell which brings alevin below all of the other methods, when in fact it looks like it might have the highest UMI's/cell for samples that are not all dead. In general, Alevin appears to have a lot more variability. Let's look at each sample individually. 

```{r}
# number of UMI's/sample
ggplot(cell_qc_filter, aes(x = tool, y = sum, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("UMI/cell") + 
  xlab("")
```
So, it looks like across all samples, cellranger is consistently near the bottom, while kallisto is consistently near the top. Alevin fluctuates all the way from detecting the most to detecting the least... 
Alevin-fry seems to always be in the middle and it looks like the `--sketch` mode leads to increased UMI/cell with the same number of cells (see previous plots looking at the number of cells).

What about genes/cell? Are the same trends consistent?

```{r}
cell_qc_filter %>%
  mutate(tool = fct_reorder(tool, detected, .fun = 'mean')) %>%
  ggplot(aes(x = reorder(tool, detected), y = detected, fill = tool)) + 
  geom_boxplot() + 
  theme_classic() + 
  ylab("Number of Genes detected/Cell") + 
  xlab("")
```


```{r}
ggplot(cell_qc_filter, aes(y = detected, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) + 
  theme_classic() + 
  ylab("Genes Detected/Cell") +
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
  
```

So we see a lot more variation in the genes detected/cell across tools than we did in the UMI/cell. Alevin is highest in the same 126 and 127 samples and cellranger consistently seems to be on the low end. Alevin-fry in `--sketch` mode again seems to get more coverage than alevin-fry without `--sketch` mode. 

Previously Josh had found that Kallisto detects more UMI's and more genes/cell but that doesn't always seem to be the case. It looks like in 126 and 127, Alevin and Alevin-fry `--sketch` are higher. But again, cellranger is keeping more cells than the other tools in those 2 samples. 

What if we just look at cells that are found in all of the tools? 

```{r}
cell_counts <- cell_qc_filter %>%  
  dplyr::count(cell_id, sample)

common_cells <- cell_counts %>%
  dplyr::filter(n == 5) %>%
  dplyr::pull(cell_id)

cell_qc_common <- cell_qc_filter %>%
  dplyr::filter(
    (cell_id %in% common_cells) 
  )
```

Now what happens to our mito content, UMI/cell, and genes/cell? 

```{r}
# mito comparison across shared cells only of all 5 runs
ggplot(cell_qc_common, aes(y = subsets_mito_percent, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) + 
  theme_classic() + 
  ylab("% Mito /Cell") + 
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
```
```{r}
# number of UMI's/sample in shared cells only 
ggplot(cell_qc_common, aes(x = tool, y = sum, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ylab("UMI/cell") + 
  xlab("")
```
```{r}
# genes detected/cell in shared cells only 
ggplot(cell_qc_common, aes(y = detected, fill = tool)) + 
  geom_boxplot() + 
  facet_wrap(~sample) + 
  theme_classic() + 
  ylab("Genes Detected/Cell") +
  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
  
```
Wow, so everything is very consistent when you look at only shared cells except for the genes/cell. Now, we actually see that alevin-fry `--sketch` captures the highest number of genes/cell in shared cells only, with cellranger, alevin, and alevin-fry without `--sketch` all capturing around the same. 

Now that we've looked at the stats for the shared cells, let's look and see how many cells are shared across which tools.  

```{r}
# make a table with information about each cell identified by a tool and how many other tools 
# that cell is identified in
cell_counts <- cell_qc_filter %>%
  dplyr::select(tool, cell_id, sum, detected, sample) %>%
  left_join(cell_counts)
```

```{r}
cell_counts <- cell_counts %>%
  group_by(tool, n) %>%
  tally(name = "overlapping_cells") %>%
  dplyr::rename("tools_detected" = "n")


ggplot(cell_counts, aes(x = tools_detected, y = overlapping_cells, fill = tool)) + 
  geom_col(position = "dodge") + 
  theme_classic()
```

I'm not sure I really like this representation, but I think it tells us a little bit of information. It looks like Cellranger identifies a group of unique cells that none of the other tools are able to identify. I wonder why this is the case? It looks like alevin also identifies a small group of unique cells. 
In general there are a few cells that are only found in 2 of the tools, but it looks like for the most part Alevin-fry, Alevin-fry-sketch, and Kallisto are all identifying the same cells and a subset of the cellranger cells. Alevin is a little weird with some overlap but also some unique cells. 

Let's look at the correlation of UMI's/cell across the different tools. We're only going to look at the cells that are in common. 

```{r}
# spread table for comparisons
cell_qc_common_cor <- cell_qc_common %>%
  # spread the mean expression stats to one column per caller
  tidyr::pivot_wider(id_cols = c(cell_id, sample),
                     names_from = tool,
                     values_from = sum) %>%
  # drop rows with NA values to ease correlation calculations
  tidyr::drop_na()
```

For simplicity, let's just look at correlation of each tool with cellranger
```{r}
cell_qc_common_cor %>% 
  dplyr::group_by(sample) %>%
  dplyr::summarize(
    cr_al_cor = cor(cellranger, alevin, method = "spearman"),
    cr_al_fry_cor =cor(cellranger, alevin_fry, method = "spearman"),
    cr_al_fry_sketch_cor = cor(cellranger, alevin_fry_sketch, method = "spearman"),
    cr_ka_cor = cor(cellranger, kallisto, method = "spearman")
  )
```
Overall, they all have really high correlations with cellranger, although it does look like Alevin-fry has the consistently highest correlation with kallisto consistently the lowest correlation. 

```{r}
ggplot(cell_qc_common_cor, aes(x = cellranger, y = alevin_fry)) +
  geom_point(size = 0.5, alpha = 0.1) + 
  facet_wrap(~ sample) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Cell Ranger UMI/cell", y = "Alevin Fry UMI/cell") + 
  theme_classic()
```

What about looking at the correlation of genes/cell. I think this should be relatively the same as UMI/cell. 
```{r}
gene_qc_common_cor <- cell_qc_common %>%
  # spread the mean expression stats to one column per caller
  tidyr::pivot_wider(id_cols = c(cell_id, sample),
                     names_from = tool,
                     values_from = detected) %>%
  # drop rows with NA values to ease correlation calculations
  tidyr::drop_na()
```

```{r}
gene_qc_common_cor %>% 
  dplyr::group_by(sample) %>%
  dplyr::summarize(
    cr_al_cor = cor(cellranger, alevin, method = "spearman"),
    cr_al_fry_cor =cor(cellranger, alevin_fry, method = "spearman"),
    cr_al_fry_sketch_cor = cor(cellranger, alevin_fry_sketch, method = "spearman"),
    cr_ka_cor = cor(cellranger, kallisto, method = "spearman")
  )
```
There is some slight variation, but still quite hgih. Again, it looks like Alevin fry has the best, with Kallisto having the worst. 

```{r}
ggplot(gene_qc_common_cor, aes(x = cellranger, y = alevin_fry)) +
  geom_point(size = 0.5, alpha = 0.1) + 
  facet_wrap(~ sample) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Cell Ranger Gene/cell", y = "Alevin Fry Gene/cell") + 
  theme_classic()
```
```{r}
ggplot(gene_qc_common_cor, aes(x = cellranger, y = kallisto)) +
  geom_point(size = 0.5, alpha = 0.1) + 
  facet_wrap(~ sample) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Cell Ranger Gene/cell", y = "Kallisto Gene/cell") + 
  theme_classic()
```
When actually looking at each gene, you see that kallisto starts to show some variation, where as alevin fry has a much cleaner diagonal and is more similar to cellranger for the cells that are detected by both. Another thing to note, which is shown in the boxplots previously, the ranges for these comparisons are not always the same. So yes, there is strong correlation, but cellranger still has a lower range of gene/cell than both Alevin-fry and kallisto.   

### Per Gene QC

I'm going to switch gears a little and look at the gene features to see if we see some similar results there. First, we need to start by calculating the perFeature QC metrics. 

```{r}
# filter and add Feature QC
alevin_sces <- alevin_sces %>%
  #purrr::map(~ .x[, colnames(.x) %in% common_cells]) %>%
  purrr::map(scater::addPerFeatureQC)

alevin_fry_sces <- alevin_fry_sces %>%
  #purrr::map(~ .x[, colnames(.x) %in% common_cells]) %>%
  purrr::map(scater::addPerFeatureQC)

alevin_fry_sketch_sces <- alevin_fry_sketch_sces %>%
  #purrr::map(~ .x[, colnames(.x) %in% common_cells]) %>%
  purrr::map(scater::addPerFeatureQC)

kallisto_sces <- kallisto_sces %>%
  #purrr::map(~ .x[, colnames(.x) %in% common_cells]) %>%
  purrr::map(scater::addPerFeatureQC)

cellranger_sces <- cellranger_sces %>%
  #purrr::map(~ .x[, colnames(.x) %in% common_cells]) %>%
  purrr::map(scater::addPerFeatureQC)
```

```{r}
genedata_to_df <- function(sce){
  # extract the feature (gene) summary data from a SCE, 
  # convert to data frame and move gene id to a column
  as.data.frame(rowData(sce)) %>%
  tibble::rownames_to_column(var = "gene_id")
}

alevin_feature_qc <- alevin_sces %>%
  purrr::map_df(genedata_to_df, .id = "quant_id")

alevin_fry_feature_qc <- alevin_fry_sces %>%
  purrr::map_df(genedata_to_df, .id = "quant_id")

alevin_fry_sketch_feature_qc <- alevin_fry_sketch_sces %>%
  purrr::map_df(genedata_to_df, .id = "quant_id")

kallisto_feature_qc <- kallisto_sces %>%
  purrr::map_df(genedata_to_df, .id = "quant_id")

cellranger_feature_qc <- cellranger_sces %>%
  purrr::map_df(genedata_to_df, .id = "quant_id")


feature_qc <- dplyr::bind_rows(
  alevin = alevin_feature_qc,
  alevin_fry = alevin_fry_feature_qc, 
  alevin_fry_sketch = alevin_fry_sketch_feature_qc,
  kallisto = kallisto_feature_qc, 
  cellranger = cellranger_feature_qc,
  .id = "tool"
) %>%
  dplyr::left_join(quant_info,
                   by = c("tool" = "tool", 
                          "quant_id" = "quant_dir")) %>%
  dplyr::filter(sample != "SCPCR000003")
```

```{r}
gene_counts <- feature_qc %>% 
  # remove genes that have a low frequency of being detected
  dplyr::filter(detected >= 5.0) %>%
  dplyr::count(gene_id, sample)

common_genes <- gene_counts %>%
  dplyr::filter(n == 5) %>%
  dplyr::pull(gene_id)

feature_qc_common <- feature_qc %>%
  dplyr::filter(
    (gene_id %in% common_genes) 
  )
```


```{r}
# make a table with information about each gene identified by a tool and how many other tools 
# that cell is identified in
gene_counts <- feature_qc %>%
  dplyr::filter(detected >= 5.0) %>%
  dplyr::select(tool, gene_id, mean, detected, sample) %>%
  left_join(gene_counts)
```


```{r}
gene_counts <- gene_counts %>%
  group_by(tool, n) %>%
  tally(name = "overlapping_genes") %>%
  dplyr::rename("tools_detected" = "n")


ggplot(gene_counts, aes(x = tools_detected, y = overlapping_genes, fill = tool)) + 
  geom_col(position = "dodge") + 
  theme_classic()
```
This plot isn't as informative as when looking at shared cells across each sample. In general it looks like the majority of the genes are identified across all samples, so let's actually look at a per cell level for this which might give us a much better picture. 


```{r}
# spread table for comparisons
feature_compare <- feature_qc %>%
  dplyr::filter(gene_id %in% detected_features) %>%
  # spread the mean expression stats to one column per caller
  tidyr::pivot_wider(id_cols = c(gene_id, sample),
                     names_from = tool,
                     values_from = mean) %>%
  # drop rows with NA values to ease correlation calculations
  tidyr::drop_na()
```

```{r}
feature_compare %>% 
  dplyr::group_by(sample) %>%
  dplyr::summarize(
    cr_al_cor = cor(cellranger, alevin, method = "spearman"),
    cr_al_fry_cor =cor(cellranger, alevin_fry, method = "spearman"),
    cr_al_fry_sketch_cor = cor(cellranger, alevin_fry_sketch, method = "spearman"),
    cr_ka_cor = cor(cellranger, kallisto, method = "spearman")
  )
```
So here we have correlation of mean expression/gene and the correlations here are much more varied than when looking at UMI/cell. We see that the highest correlation is between Alevin and Alevin-fry (without sketch), while Kallisto has the lowest correlation. It appears that the `--sketch` mode, although increases time (shown elsewhere), doesn't appear to have an increased advantage over Alevin-fry in terms of quantification. 

```{r}
ggplot(feature_compare, aes(x = cellranger, y = alevin_fry)) +
  geom_point(size = 0.5, alpha = 0.1) + 
  facet_wrap(~ sample) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Cell Ranger Mean Gene Expression", y = "Alevin-fry Mean Gene Expression") + 
  theme_classic()
```
```{r}
ggplot(feature_compare, aes(x = cellranger, y = kallisto)) +
  geom_point(size = 0.5, alpha = 0.1) + 
  facet_wrap(~ sample) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "Cell Ranger Mean Gene Expression", y = "Kallisto Mean Gene Expression") + 
  theme_classic()
```
Interestingly, Alevin-fry has a higher correlation than Kallisto, the pattern is much different. The mean gene expression tends to be higher in genes in Alevin-fry than in Cellranger. In kallisto, there are genes that are higher in both kallisto and cellranger. 

In conclusion, it appears that there is some variability between the tools in cell barcode assignment, with cellranger assigning what appears to be more cells, but a lower number of UMI/cells and gene/cells in compared to Kallisto and Alevin-fry. For the most part, there is high correlation of each tool with cellranger, but Alevin-Fry/Alevin have the highest and Kallisto the lowest. Although Kallisto has the fastest compute time, Alevin-Fry is still much faster than cellranger and uses a fraction of the memory while obtaining similar quantification information. The biggest discrepancy is in the number of cells that are identified, so perhaps it is worth looking into the other resolution methods used in Alevin-fry, as we are using the "full" resolution method here. 

Additionally, we will need to incorporate snRNA-seq and other 10X modalities into this analysis (like 10Xv2 and 5') to see if our results are consistent. 

## SessionInfo
```{r}
sessionInfo()
```
