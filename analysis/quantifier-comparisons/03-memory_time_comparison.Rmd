---
title: "ScPCA Benchmarking: Run Time and Memory Comparison"
output: html_notebook
---

This notebook compares the run time and memory footprint of pre-processing 4 scRNA-seq samples across 4 different tools. We used cellranger v6, kallisto, alevin, and alevin-fry. 
Alevin-fry was used with and without `--sketch` mode and/or `--unfiltered-pl`. 
The purpose of this notebook is to gain an understanding on how much time and memory each of the tools uses. 
We are starting with looking at 4 scRNA-seq samples and 2 snRNA-seq samples.
The snRNA-seq samples have been run with 2 different indices, cDNA and pre-mRNA.


```{r libary import}
library(tidyverse)
library(rjson)
```


```{r}
## set up file paths 
base_dir <- getwd()

logs_dir <- file.path(base_dir, "nextflow_logs")
alevin_dir <- file.path(logs_dir, "alevin-quant", "alevin")
kallisto_dir <- file.path(logs_dir, "kallisto-quant")
cellranger_dir <- file.path(logs_dir, "cellranger-quant")
alevin_fry_dir <- file.path(logs_dir, "alevin-quant", "alevin-fry")
alevin_fry_sketch_dir <- file.path(logs_dir, "alevin-quant", "alevin-fry-sketch")
alevin_fry_unfiltered_dir <- file.path(logs_dir, "alevin-quant", "alevin-fry-unfiltered")
alevin_fry_unfiltered_sketch_dir <- file.path(logs_dir, "alevin-quant", "alevin-fry-unfiltered-sketch")
```

```{r}
# setup directories and paths for kallisto logs 

data_dir <- file.path(base_dir, 'data', 'quants')
dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)

kallisto_quants_dir <- file.path(data_dir, 'kallisto')
dir.create(kallisto_quants_dir, recursive = TRUE, showWarnings = FALSE)
quant_s3_kallisto <- 's3://nextflow-ccdl-results/scpca/kallisto-quant'
```


```{r}
# get log files from S3
# include json file from each kallisto run 
samples <- c('SCPCR000003', 'SCPCR000006', 'SCPCR000118', 'SCPCR000119', 'SCPCR000126', 'SCPCR000127')

includes <- stringr::str_glue("--include \"*/{samples}\"")

sync_call <- paste('aws s3 sync', quant_s3_kallisto, kallisto_quants_dir,
                   includes,'--exclude "*/bus/*"', '--exclude "*/counts/*"',
                   '--include "*/bus/run_info.json"')
system(sync_call, ignore.stdout = TRUE)
```


For each run, we asked nextflow to ouput a trace.txt txt file using the 
-with-trace flag. These [trace files](https://www.nextflow.io/docs/latest/tracing.html) 
include important information about each process such as run time and memory. 

## Alevin

```{r}
make_trace_df <- function(workdir, file_ext){
  setwd(workdir)
  workdir_files <- list.files(workdir)
  workdir_list <- lapply(workdir_files, function(x) readr::read_tsv(x))
  names(workdir_list) <- gsub(file_ext, "", workdir_files)
  workdir_df <- bind_rows(workdir_list)
}
```


```{r}
alevin_trace_df <- make_trace_df(alevin_dir, ".trace.txt")
alevin_trace_df$Tool <- "Alevin"
```

## Alevin Fry 

```{r}
alevin_fry_trace_df <- make_trace_df(alevin_fry_dir, ".trace.txt")
alevin_fry_trace_df$Tool <- "Alevin-Fry"
```


```{r}
# alevin fry sketch
alevin_fry_sketch_trace_df <- make_trace_df(alevin_fry_sketch_dir, ".sketch.trace.txt")
alevin_fry_sketch_trace_df$Tool = "Alevin-Fry-Sketch"
```


```{r}
# alevin fry unfiltered
alevin_fry_unfiltered_trace_df <- make_trace_df(alevin_fry_unfiltered_dir, ".unfiltered.trace.txt")
alevin_fry_unfiltered_trace_df$Tool = "Alevin-Fry-Unfiltered"
```


```{r}
# alevin fry unfiltered sketch
alevin_fry_unfiltered_sketch_trace_df <- make_trace_df(alevin_fry_unfiltered_sketch_dir, ".unfiltered.sketch.trace.txt")
alevin_fry_unfiltered_sketch_trace_df$Tool = "Alevin-Fry-Unfiltered-Sketch"
```


## Kallisto

```{r}
kallisto_trace_df <- make_trace_df(kallisto_dir, ".trace.txt")
kallisto_trace_df$Tool = "Kallisto"
```

## CellRanger

```{r}
cellranger_trace_df <- make_trace_df(cellranger_dir, ".trace.txt")
cellranger_trace_df$Tool = "Cellranger"
```


## Run time comparison 

```{r}
## first make a joint dataframe
all_trace <- bind_rows(list(alevin_trace_df, alevin_fry_trace_df,
                            alevin_fry_sketch_trace_df, alevin_fry_unfiltered_trace_df,
                            alevin_fry_unfiltered_sketch_trace_df, kallisto_trace_df, 
                            cellranger_trace_df))
all_trace
```
We need to do some manipulation to this data frame to the peak_rss and 
realtime columns so they are all in the right units and of class(numeric)
for plotting.

```{r}
## now separate the columns by process and sample_id to make cleaner for plotting
all_trace <- all_trace %>% 
  separate(name, into = c("process", "sample_id"), sep = " ") %>%
  separate(sample_id, into = c("sample_id", "index_name"), sep = "-")

all_trace$sample_id = gsub("[()]", "", all_trace$sample_id)
all_trace$index_name = gsub("[()]", "", all_trace$index_name)

## make separate column for either GB or MB for peak_rss
all_trace <- all_trace %>%
  separate(peak_rss, into = c("peak_rss", "rss_units"), sep = " ") %>%
  mutate(peak_rss = as.numeric(peak_rss)) %>%
  mutate(peak_rss = case_when(rss_units == "MB" ~ (peak_rss/1000),
         TRUE ~ peak_rss))
  

## convert from hours, minutes, to seconds
all_trace <- all_trace %>% 
  mutate(hours = str_extract(all_trace$duration, "\\d+h"),
         minutes = str_extract(all_trace$duration, "\\d+m"),
         seconds = str_extract(all_trace$duration, "\\d+s")) %>%
  mutate(hours = gsub("h", "", hours),
         minutes = gsub("m", "", minutes),
         seconds = gsub("s", "", seconds)) %>%
  mutate(hours = as.numeric(hours),
         minutes = as.numeric(minutes), 
         seconds = as.numeric(seconds))
all_trace$hours[is.na(all_trace$hours)] = 0
all_trace$minutes[is.na(all_trace$minutes)] = 0
all_trace$seconds[is.na(all_trace$seconds)] = 0

all_trace <- all_trace %>%
  mutate(total_time = hours*60 + minutes + seconds/60)
all_trace
```
We will also want to compare the number of reads in each sample to the memory/
runtime so let's add in that information. 

```{r}
## add in number of reads/ sample 
## found in individual logfiles from kallisto 
single_cell_samples <- c('SCPCR000003', 'SCPCR000006', 'SCPCR000126', 'SCPCR000127')

all_trace$Reads = 0
for (sample in samples) {
  if(sample %in% single_cell_samples){
    # only samples that are single cell were run with this index
    sample_file <- paste0(sample, "-txome_k31")
  }
  else {
    # single nuclei samples have the spliced intron ending for the file name
    sample_file <- paste0(sample, "-spliced_intron_txome_k31")
  }
  # read in run_info.json log from each kallisto run 
  log <- fromJSON(file = file.path(kallisto_quants_dir, sample_file, 
                                 "bus", "run_info.json"))
  #n_processed gives amount of reads for each sample
  all_trace[which(all_trace$sample_id == sample), "Reads"] <- log$n_processed
}

```


Let's also add in a column about sequencing technology 
```{r}
# add in library metadata for each sample  
library_data_dir <- file.path(base_dir, 'sample-info')
dir.create(library_data_dir, recursive = TRUE, showWarnings = FALSE)
sample_info_dir_s3 <- 's3://ccdl-scpca-data/sample_info'

# grab library metadata from location in s3
sync_call <- paste('aws s3 sync', sample_info_dir_s3, library_data_dir,
                   '--exclude "*"', 
                   '--include "*scpca-library-metadata.tsv"')
system(sync_call, ignore.stdout = TRUE)
```

```{r}
# read in sample metadata
library_df <- readr::read_tsv(file.path(library_data_dir, "scpca-library-metadata.tsv"))
```

```{r}
# filter data frame with information that might be relevant for looking at these samples
select_metadata_df <- library_df %>%
  dplyr::select(scpca_run_id, scpca_sample_id, seq_unit, technology)
```

```{r}
# make combined df with technology and seq unit 
all_trace_comb <- all_trace %>%
  left_join(select_metadata_df, by = c("sample_id" = "scpca_run_id"))
```


```{r}
# create a summary data frame that merges total time and memory for all steps
summary_stats_df <- all_trace_comb %>% 
  group_by(Tool, seq_unit, sample_id, index_name) %>% 
  summarise(total_time = sum(total_time), 
            total_rss = sum(peak_rss), 
            Reads = mean(Reads))
summary_stats_df
```

## Time and Memory with Sample ID on x-axis

```{r}
ggplot(summary_stats_df %>%
         dplyr::filter(seq_unit == "cell"), 
       aes(x = sample_id, y = total_time, color = Tool)) +
  geom_point(size = 2) + 
  theme_classic() + 
  xlab("") +
  ylab("Run Time (Minutes)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
ggplot(summary_stats_df %>%
         dplyr::filter(seq_unit == "nucleus"), 
       aes(x = sample_id, y = total_time, color = Tool)) +
  geom_point(size = 2) + 
  theme_classic() + 
  xlab("") +
  ylab("Run Time (Minutes)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
# another way to visualize it if we want to group all the samples together
summary_stats_df %>%
  mutate(Tool = fct_reorder(Tool, total_time, .fun = 'median')) %>%
  ggplot(aes(x = reorder(Tool, total_time), y = total_time)) + 
  geom_boxplot() + 
  geom_jitter(mapping = aes(color = sample_id)) +
  theme_classic() +
  xlab("") +
  ylab("Run Time (Minutes)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
ggplot(summary_stats_df, aes(x = sample_id, y = total_rss, color = Tool)) + 
  geom_point(size = 2) + 
  theme_classic() + 
  xlab("") +
  ylab("Memory (GB)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
```{r}
# Just alevin tools to compare better across those 
# run time
alevin_tools = c("Alevin","Alevin-Fry", "Alevin-Fry-Unfiltered", "Alevin-Fry-Unfiltered-Sketch")

ggplot(summary_stats_df %>%
         dplyr::filter(Tool %in% alevin_tools), 
       aes(x = sample_id, y = total_time, color = Tool)) +
  geom_point(size = 2) + 
  theme_classic() + 
  xlab("") +
  ylab("Run Time (Minutes)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
ggplot(summary_stats_df %>%
         dplyr::filter(Tool %in% alevin_tools), 
       aes(x = sample_id, y = total_rss, color = Tool)) + 
  geom_point(size = 2) + 
  theme_classic() + 
  xlab("") +
  ylab("Memory (GB)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r}
# another way to visualize it if we want to group all the samples together
summary_stats_df %>%
  mutate(Tool = fct_reorder(Tool, total_rss, .fun = 'median')) %>%
  ggplot(aes(x = reorder(Tool, total_rss), y = total_rss)) +
  geom_violin() + 
  geom_jitter(mapping = aes(color = Tool)) +
  theme_classic() +
  xlab("") +
  ylab("Memory (GB)") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


After looking at the runtime and memory used independently of any other 
observations, we can see that Kallisto is consistently the fastest, although 
alevin-fry in --sketch mode is also quite fast. This is probably because it is
supposed to mimic kallisto. Cellranger is consistently the slowest by almost 
3x more than the next fastest, Alevin/Alevin-fry without --sketch mode which 
are both somewhere in the middle.

In terms of memory, Alevin-fry with/without --sketch uses the least amount
of memory using < 5 total GB of memory on all 4 samples. Kallisto is also 
consistently using ~ 11 GB of memory for all 4 samples. Alevin and cellranger
are higher than this, with Alevin requiring the most amount of memory closer
to ~ 20 GB of memory per sample. 

Next, let's see if the number of reads in each fastq file impacts the runtime
and/or memory usage. 

## Time and Memory vs. # of Reads 

```{r}
# run time vs. # of fastq reads
ggplot(summary_stats_df, aes(x = Reads, y = total_time, color = Tool)) +
  geom_point(size = 2, mapping = aes(shape = sample_id)) + 
  theme_classic() + 
  xlab("Number of Reads") +
  ylab("Run Time (Minutes)")
```
```{r}
# remove cellranger and look at the rest of the tools 
ggplot(summary_stats_df %>%
         dplyr::filter(Tool != "Cellranger"), 
       aes(x = Reads, y = total_time, color = Tool)) +
  geom_point(size = 2, mapping = aes(shape = sample_id)) + 
  theme_classic() + 
  xlab("Number of Reads") +
  ylab("Run Time (Minutes)")
```


```{r}
ggplot(summary_stats_df, aes(x = Reads, y = total_rss, color = Tool)) + 
  geom_point(size = 2, mapping = aes(shape = sample_id)) + 
  theme_classic() + 
  xlab("Number of Reads") +
  ylab("Memory (GB)")
```
```{r}
ggplot(summary_stats_df %>%
         dplyr::filter(Tool != "Kallisto"),
       aes(x = Reads, y = total_rss, color = Tool)) + 
  geom_point(size = 2, mapping = aes(shape = sample_id)) + 
  theme_classic() + 
  xlab("Number of Reads") +
  ylab("Memory (GB)")
```


From this, it looks like there is no effect on the number of reads found in 
the fastq files to the memory that is required for any of the processes, except
maybe with alevin-fry both with/without --sketch. It appears that the memory
usage almost doubles from ~2-3 GB to closer to ~5-6 GB. Even though memory
usage doesn't appear to be that affected, run time does appear to be affected
by the number of reads in the sample if running on cellranger. As the number
of reads increase, there is a sharp increase in the run time for cellranger, 
where as that is not the case for the other tools.

It is clear that cellranger has both the biggest footprint in time and memory 
usage. However, the question remains how the count matrices compare across the 
tools, and whether the quantification performed by alevin and kallisto is 
comparable to cellranger even with a lower footprint. 

## Session Info

```{r}
sessionInfo()
```

